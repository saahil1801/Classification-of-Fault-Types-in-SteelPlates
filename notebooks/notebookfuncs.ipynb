{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from numba import NumbaDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=NumbaDeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Standardization and Encoding\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Modelling\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ignore warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set : (19219, 35)\n",
      "Test data set : (12814, 28)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('playground-series-s4e3/train.csv')\n",
    "df_test = pd.read_csv('playground-series-s4e3/test.csv')\n",
    "\n",
    "\n",
    "print('Train data set : {}'.format(df_train.shape))\n",
    "print('Test data set : {}'.format(df_test.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define numerical features and targets\n",
    "numerical_features = [\n",
    "    'Sum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'TypeOfSteel_A400',\n",
    "    'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index',\n",
    "    'Square_Index', 'Outside_X_Index', 'Edges_X_Index', 'Edges_Y_Index',\n",
    "    'Outside_Global_Index', 'LogOfAreas', 'Log_X_Index',\n",
    "    'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas'\n",
    "]\n",
    "target_features = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df_train: ['id', 'Sum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas', 'Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults', 'X', 'Y', 'Luminosity', 'Area_Perimeter_Ratio']\n",
      "Columns in df_test: ['id', 'Sum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas', 'X', 'Y', 'Luminosity', 'Area_Perimeter_Ratio']\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in df_train:\", df_train.columns.tolist())\n",
    "print(\"Columns in df_test:\", df_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess the data by engineering features and dropping unnecessary columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The preprocessed DataFrame with engineered features and unnecessary columns removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Feature Engineering\n",
    "    columns_in_df = df.columns\n",
    "    print(\"Columns in DataFrame:\", columns_in_df)\n",
    "    try:\n",
    "        df['X'] = df['X_Maximum'] - df['X_Minimum']\n",
    "        df['Y'] = df['Y_Maximum'] - df['Y_Minimum']\n",
    "        df['Luminosity'] = df['Maximum_of_Luminosity'] - df['Minimum_of_Luminosity']\n",
    "        df['Area_Perimeter_Ratio'] = df['Pixels_Areas'] / (df['X_Perimeter'] + df['Y_Perimeter'])\n",
    "        \n",
    "        # Drop original columns\n",
    "        df = df.drop(['X_Maximum', 'X_Minimum', 'Y_Maximum', 'Y_Minimum', 'Maximum_of_Luminosity', 'Minimum_of_Luminosity',\n",
    "                    'Pixels_Areas', 'X_Perimeter', 'Y_Perimeter'], axis=1)\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing column: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline():\n",
    "    # Preprocessing pipeline\n",
    "\n",
    "    \"\"\"\n",
    "    Build a machine learning pipeline that includes preprocessing, feature selection, and model training.\n",
    "\n",
    "    Returns:\n",
    "    Pipeline: A scikit-learn pipeline object that includes preprocessing, feature selection, and the XGBoost classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', RobustScaler())\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Complete pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selection', SelectKBest(score_func=f_classif, k='all')),\n",
    "        ('model', XGBClassifier(learning_rate=0.01, n_estimators=300, objective='binary:logistic'))\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test):\n",
    "    # Define the pipeline\n",
    "    pipeline = build_pipeline()\n",
    "    \n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"F1 Score: \", metrics.f1_score(y_test, y_pred))\n",
    "    print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n",
    "    print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in DataFrame: Index(['id', 'X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum',\n",
      "       'Pixels_Areas', 'X_Perimeter', 'Y_Perimeter', 'Sum_of_Luminosity',\n",
      "       'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer',\n",
      "       'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness',\n",
      "       'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index',\n",
      "       'Edges_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas',\n",
      "       'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index',\n",
      "       'SigmoidOfAreas', 'Pastry', 'Z_Scratch', 'K_Scatch', 'Stains',\n",
      "       'Dirtiness', 'Bumps', 'Other_Faults'],\n",
      "      dtype='object')\n",
      "Columns in DataFrame: Index(['id', 'X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum',\n",
      "       'Pixels_Areas', 'X_Perimeter', 'Y_Perimeter', 'Sum_of_Luminosity',\n",
      "       'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer',\n",
      "       'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness',\n",
      "       'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index',\n",
      "       'Edges_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas',\n",
      "       'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index',\n",
      "       'SigmoidOfAreas'],\n",
      "      dtype='object')\n",
      "Processing target: Pastry\n",
      "Accuracy:  0.8359347901491502\n",
      "F1 Score:  0.3668005354752343\n",
      "Precision:  0.26095238095238094\n",
      "Recall:  0.6171171171171171\n",
      "Processing target: Z_Scratch\n",
      "Accuracy:  0.9373916059660077\n",
      "F1 Score:  0.5902383654937571\n",
      "Precision:  0.47706422018348627\n",
      "Recall:  0.7738095238095238\n",
      "Processing target: K_Scatch\n",
      "Accuracy:  0.9590704127644815\n",
      "F1 Score:  0.8894095595126523\n",
      "Precision:  0.8541854185418541\n",
      "Recall:  0.927663734115347\n",
      "Processing target: Stains\n",
      "Accuracy:  0.9816163718348943\n",
      "F1 Score:  0.7336683417085427\n",
      "Precision:  0.6160337552742616\n",
      "Recall:  0.906832298136646\n",
      "Processing target: Dirtiness\n",
      "Accuracy:  0.9079084287200833\n",
      "F1 Score:  0.24466571834992887\n",
      "Precision:  0.15302491103202848\n",
      "Recall:  0.6099290780141844\n",
      "Processing target: Bumps\n",
      "Accuracy:  0.7643080124869928\n",
      "F1 Score:  0.5219838199085474\n",
      "Precision:  0.5131396957123098\n",
      "Recall:  0.5311381531853973\n",
      "Processing target: Other_Faults\n",
      "Accuracy:  0.663891779396462\n",
      "F1 Score:  0.4739413680781759\n",
      "Precision:  0.5221291866028708\n",
      "Recall:  0.43389662027833004\n"
     ]
    }
   ],
   "source": [
    "df_train = preprocess_data(df_train)\n",
    "df_test = preprocess_data(df_test)\n",
    "    \n",
    "X = df_train.drop(target_features + ['id'], axis=1)\n",
    "y = df_train[target_features]\n",
    "    \n",
    "for i, target in enumerate(target_features):\n",
    "        print(f\"Processing target: {target}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y[target], test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Oversample\n",
    "        smote = SMOTE(sampling_strategy='auto')\n",
    "        X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        train_and_evaluate(X_smote, y_smote, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
